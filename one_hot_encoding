1. import pandas as pd

data = pd.DataFrame({'fruit': ['apple', 'orange', 'banana', 'banana', 'apple']})
hot_encoded = pd.get_dummies(data['fruit'])
print(hot_encoded)

   apple  banana  orange
0      1       0       0
1      0       0       1
2      0       1       0
3      0       1       0
4      1       0       0
####################################################
2. from keras.utils import to_categorical

labels = ['cat', 'dog', 'bird', 'dog', 'cat']
hot_encoded = to_categorical(labels)
print(hot_encoded)

array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.],
       [0., 1., 0.],
       [1., 0., 0.]], dtype=float32)
#####################################################
3. from sklearn.preprocessing import OneHotEncoder
import numpy as np

data = np.array([['red'], ['green'], ['blue'], ['green']])
encoder = OneHotEncoder()
hot_encoded = encoder.fit_transform(data)
print(hot_encoded.toarray())

array([[0., 1., 0.],
       [0., 0., 1.],
       [1., 0., 0.],
       [0., 0., 1.]])
#####################################################
원-핫 인코딩은 범주형 데이터를 기계 학습 모델에 쉽게 사용할 수 있는 형식으로 변환하는 데 사용되는 기술입니다.
범주형 데이터는 색상, 제품 유형 또는 국가 이름과 같은 특정 그룹 또는 범주로 구분된 데이터를 나타냅니다.
원-핫 인코딩은 범주형 데이터를 일련의 이진(0/1) 값으로 나타내는 방법입니다.
각 범주는 길이가 n인 이진 벡터로 표시됩니다. 여기서 n은 범주의 총 수입니다. 이 이진 벡터에서 한 비트만 1이고 다른 모든 비트는 0입니다.
예를 들어 사과, 바나나, 오렌지의 세 가지 범주가 있는 범주형 변수 "과일"이 있다고 가정해 보겠습니다. 다음과 같이 원-핫 인코딩을 사용하여 이 변수를 나타낼 수 있습니다.
사과: [1, 0, 0]
바나나: [0, 1, 0]
주황색: [0, 0, 1]
이 표현에서 각 관측값은 원래 변수의 고유한 범주 수와 길이가 같은 벡터로 표현됩니다.
벡터의 1 값은 관측치에 해당 범주가 있음을 나타내고 0 값은 해당 범주가 없음을 나타냅니다.
원-핫 인코딩은 기계 학습 모델에서 쉽게 사용할 수 있는 방식으로 범주형 변수를 나타낼 수 있기 때문에 유용합니다. 
#####################################################
원-핫 인코딩(One-Hot Encoding)의 한계
이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다.
다른 표현으로는 벡터의 차원이 늘어난다고 표현합니다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다.
가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다.
다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데 이는 저장 공간 측면에서는 매우 비효율적인 표현 방법입니다.
또한 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점이 있습니다.
예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서
원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다.
이때 원-핫 벡터로는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다.
좀 더 극단적으로는 강아지, 개, 냉장고라는 단어가 있을 때 강아지라는 단어가 개와 냉장고라는 단어 중 어떤 단어와 더 유사한지도 알 수 없습니다.
단어 간 유사성을 알 수 없다는 단점은 검색 시스템 등에서는 문제가 될 소지가 있습니다.
가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 합시다. 제대로 된 검색 시스템이라면,
'삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트 하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 합니다.
하지만 단어간 유사성을 계산할 수 없다면, '게스트 하우스'와 '료칸'과 '호텔'이라는 연관 검색어를 보여줄 수 없습니다.
이러한 단점을 해결하기 위해 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법으로 크게 두 가지가 있습니다.
첫째는 카운트 기반의 벡터화 방법인 LSA(잠재 의미 분석), HAL 등이 있으며, 둘째는 예측 기반으로 벡터화하는 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다.
그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 GloVe라는 방법이 존재합니다.
#####################################################
1. pandas.get_dummies
범주형 변수를 더미/지표 변수로 변환합니다.
데이터의 특성을 학습하지 않기 때문에 train 데이터에만 있고 test 데이터에는 없는 카테고리를 test 데이터에서 원핫인코딩 된 칼럼으로 바꿔주지 않는다.
간단하고 빠르게 범주형 데이터를 처리할 수 있다.
범주형 변수의 수가 많아질 경우 처리 속도가 느려질 수 있다.

2.from keras.utils import to_categorical
정수형 레이블을 one-hot encoding으로 변환합니다.
범주형 데이터를 변환할 때는 사용할 수 없다.

3.sklearn.preprocessing.OneHotEncoder
범주형 데이터를 one-hot encoding으로 변환합니다.
sklearn.preprocessing.OneHotEncoder는 train 데이터의 특성을 학습할 수 있다
sklearn.preprocessing.OneHotEncoder를 사용하여 변환된 결과는 numpy.array이기 때문에 이를 데이터프레임으로 변환하는 과정이 필요하다.
pandas.get_dummies보다 더 많은 옵션과 기능을 제공한다.
대용량 데이터에 대한 처리가 가능하다.










       
